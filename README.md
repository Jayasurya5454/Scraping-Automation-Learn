# Scraped Companies

Welcome to the **Scraped Companies** repository! This repository contains scripts and data for scraping information about the top 100 companies using AWS, GCP, and Databricks. Below you will find an overview of the project, including how to get started and how to contribute.

## Project Overview

In this project, we scraped data about the top 100 companies that utilize AWS (Amazon Web Services), GCP (Google Cloud Platform), and Databricks. The goal was to gather information on these companies for analysis and insights into their usage of these cloud platforms.

## Tools and Technologies


- **BeautifulSoup and Scrapy**: Libraries used for web scraping.

## Data Collected

The data collected includes, but is not limited to:
- Company Name
- Industry
- Location
- Revenue
- Number of Employees
- Cloud Platform Usage (AWS, GCP, Databricks)

## Getting Started

Follow these instructions to get a copy of the project up and running on your local machine.

### Prerequisites

Make sure you have the following installed on your local machine:
- Python 3.x
- Pip (Python package installer)
- BeautifulSoup
- webdriver
- Selenium
### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/scraped_companies.git
